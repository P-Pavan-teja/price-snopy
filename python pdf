import io
import boto3
import pdfplumber
import pandas as pd

_s3 = boto3.client("s3")  # IICS Secure Agent will use its IAM creds

# --- same function name, but now reads PDF directly from S3 ---
def pdf_to_text(file_path, output_path=None):
    """
    If file_path is s3://..., read PDF from S3 and return list of lines.
    If it's local, fall back to old behavior.
    """
    lines = []
    if str(file_path).startswith("s3://"):
        bucket, _, key = file_path[5:].partition("/")
        obj = _s3.get_object(Bucket=bucket, Key=key)
        pdf_bytes = obj["Body"].read()
        with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
            for page in pdf.pages:
                txt = page.extract_text()
                if txt:
                    lines.extend(txt.splitlines())
        # if caller gave output_path AND wants a text file in S3
        if output_path and str(output_path).startswith("s3://"):
            buf = io.StringIO("\n".join(lines))
            b2, _, k2 = output_path[5:].partition("/")
            _s3.put_object(Bucket=b2, Key=k2, Body=buf.getvalue().encode("utf-8"))
    else:
        # original local fallback
        with pdfplumber.open(file_path) as pdf, open(output_path, "w", encoding="utf-8") as out:
            for page in pdf.pages:
                txt = page.extract_text()
                if txt:
                    out.write(txt + "\n")
    return lines

# --- same name, but handles S3 text or in-memory lines ---
def read_text_file(file_path, lines=None):
    """
    If file_path is s3://... fetch text file from S3.
    Or, if lines are already provided from pdf_to_text, build DataFrame directly.
    """
    if lines is not None:
        return pd.DataFrame({'Line': [line.strip() for line in lines]})
    if str(file_path).startswith("s3://"):
        bucket, _, key = file_path[5:].partition("/")
        obj = _s3.get_object(Bucket=bucket, Key=key)
        content = obj["Body"].read().decode("utf-8", errors="replace").splitlines()
        return pd.DataFrame({'Line': [line.strip() for line in content]})
    else:
        with open(file_path, "r", encoding="utf-8", errors="replace") as f:
            return pd.DataFrame({'Line': [line.strip() for line in f]})

# --- write DataFrame as CSV directly to S3 ---
def write_csv_to_s3(df: pd.DataFrame, s3_uri: str):
    bucket, _, key = s3_uri[5:].partition("/")
    buf = io.StringIO()
    df.to_csv(buf, index=False)
    _s3.put_object(Bucket=bucket, Key=key, Body=buf.getvalue().encode("utf-8"), ContentType="text/csv")==
