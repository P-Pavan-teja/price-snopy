import awswrangler as wr
import zipfile
import boto3
from common_grp4 import *
import traceback
from io import BytesIO
from datetime import datetime
import pandas as pd
from pytz import timezone
import warnings
import chardet

warnings.simplefilter("ignore", UserWarning)

tz = timezone('US/Eastern')
startdt = datetime.now(tz)
load_date = startdt.date().isoformat()
format_snap_date = load_date
current_timestamp = startdt.strftime('%Y%m%d%H%M%S')
process_date = startdt.strftime('%Y%m%d')

source_cnt = 0
target_cnt = 0
count = 0

param_input_path = ''
param_load_category = ''
param_task_flowname = ''
param_account_date_position = 1
account_date = ''

s3 = boto3.resource('s3')
client_s3 = boto3.client('s3')


def move_to_reject_folder(bucket_name, zipfile_name, temp_workspace, param_reject_path, 
                         original_zip_key, temp_zip_key=None, processing_stage=''):
    """
    Move files to reject folder based on processing stage
    """
    zip_basename = zipfile_name.split('/')[-1].replace('.ZIP', '').replace('.zip', '')
    reject_base_path = f'{param_reject_path}{zip_basename}_{current_timestamp}/'
    
    # Always copy original ZIP
    s3copy(bucket_name, original_zip_key, f'{reject_base_path}original_file/')
    
    # Copy temp ZIP if it exists
    if temp_zip_key:
        s3copy(bucket_name, temp_zip_key, f'{reject_base_path}temp_zip_file/')
    
    # Copy unzipped files if they exist
    if processing_stage in ['accounting_date_failed', 'utf8_failed', 'target_move_failed']:
        unzip_files = wr.s3.list_objects(f's3://{bucket_name}/{temp_workspace}unzip/')
        for file_path in unzip_files:
            filename = file_path.split('/')[-1]
            s3copy(bucket_name, file_path, f'{reject_base_path}unzip_files/{filename}')
    
    # Copy UTF8 converted files if they exist
    if processing_stage in ['utf8_failed', 'target_move_failed']:
        utf8_files = wr.s3.list_objects(f's3://{bucket_name}/{temp_workspace}utf8/')
        for file_path in utf8_files:
            filename = file_path.split('/')[-1]
            s3copy(bucket_name, file_path, f'{reject_base_path}utf_conversion/{filename}')
    
    print(f'Files moved to reject folder: {reject_base_path}')
    
    # Clean up original locations
    wr.s3.delete_objects(f's3://{bucket_name}/{temp_workspace}')
    wr.s3.delete_objects(f's3://{bucket_name}/{original_zip_key}')


if __name__ == "__main__":
    temp_workspace = ''
    temp_zip_key = ''
    original_zip_key = ''
    
    try:
        starttime = startdt.strftime('%Y-%m-%dT%H:%M:%SZ')
        print("arg1= ", sys.argv[1])
        print("arg2= ", sys.argv[2])
        print("arg3= ", sys.argv[3])
        print("arg4= ", sys.argv[4])
        print("arg5= ", sys.argv[5])

        bucket_name = sys.argv[1]
        job_id = sys.argv[2]
        task_name = sys.argv[3]
        param_file = sys.argv[4]
        zipfile_name = sys.argv[5]

        # Reading Param file in S3
        (param_input_path,
         param_input_path_fileprefix,
         param_expected_files,
         param_reject_path,
         param_output_path,
         param_load_category,
         param_task_flowname,
         param_archive_path,
         param_trailer_processdt_location,
         param_filedelimiter,
         param_specialfilenames,
         param_trailer_grandtotal_location,
         param_specialfiledelimiter,
         param_outofscopetrailerfiles,
         param_trailer_consolidation) = extract_param_value(bucket_name, param_file)

        # Setup paths
        zip_basename = zipfile_name.split('/')[-1].replace('.ZIP', '').replace('.zip', '')
        temp_workspace = f'{param_input_path}{zip_basename}_{current_timestamp}/'
        temp_zip_key = f'{temp_workspace}{zipfile_name.split("/")[-1]}'
        original_zip_key = zipfile_name

        print("="*50)
        print("STARTING ETL PIPELINE")
        print("="*50)

        # Step 1: Copy ZIP file to temporary workspace
        print("Step 1: Copying ZIP file to temporary workspace...")
        
        s3copy(bucket_name, zipfile_name, temp_zip_key)
        print(f'INFO: Successfully copied ZIP file to {temp_zip_key}')

        # Step 2: Unzip files in temporary workspace
        print("Step 2: Unzipping files in temporary workspace...")
        
        zipped_file = client_s3.get_object(Bucket=bucket_name, Key=temp_zip_key)
        buffer = BytesIO(zipped_file["Body"].read())
        z = zipfile.ZipFile(buffer)
        
        unzip_count = 0
        for filepath in z.namelist():
            filename = filepath.split('/')[-1]
            unzip_count += 1
            s3.meta.client.upload_fileobj(z.open(filepath), bucket_name, Key=f'{temp_workspace}unzip/{filename}')
        
        print(f'INFO: Successfully unzipped {unzip_count} files to {temp_workspace}unzip/')

        # Step 3: Extract accounting date from trailer file
        print("Step 3: Extracting accounting date from trailer file...")
        
        trailer_filename = f'{param_trailer_consolidation}'
        trailer_key = f'{temp_workspace}unzip/{trailer_filename}'
        
        trailer_obj = client_s3.get_object(Bucket=bucket_name, Key=trailer_key)
        trailer_content = trailer_obj['Body'].read()
        
        lines = trailer_content.decode('utf-16').splitlines()
        last_record_utf8 = lines[-1].encode('utf-8')
        data = str(last_record_utf8)
        trailer_df = pd.DataFrame([x.split(param_filedelimiter) for x in data.split('\n')])
        account_date_re = trailer_df.iloc[:, [int(param_account_date_position)]].values[0]
        account_date = str(account_date_re)[2:-2].replace('-', '')
        account_date = account_date.split(' ')[0]
        
        print(f'INFO: Successfully extracted accounting date: {account_date}')

        # Step 4: Convert files to UTF-8
        print("Step 4: Converting files to UTF-8...")
        
        format_snap_date = datetime.strptime(account_date, '%Y%m%d').date().isoformat()
        utf8_workspace = f'{temp_workspace}utf8/'
        
        # Get list of files to convert
        s3_results = wr.s3.list_objects(f's3://{bucket_name}/{temp_workspace}unzip/')
        conversion_count = 0
        
        for file_path in s3_results:
            filename = file_path.split('/')[-1]
            
            # Read file and detect encoding
            obj = s3.Object(bucket_name, file_path)
            try:
                bytes_data = obj.get()['Body'].read(1000).decode('UTF-8')
                object_content = obj.get()['Body'].read().decode('UTF-8')
            except UnicodeDecodeError:
                bytes_data = obj.get()['Body'].read(1000)
                result = chardet.detect(bytes_data)
                encode = next(iter(result.values()))
                object_content = obj.get()['Body'].read().decode(encode)
            
            # Write UTF-8 version
            utf8_key = f'{utf8_workspace}{filename}'
            client_s3.put_object(Body=object_content, Bucket=bucket_name, Key=utf8_key)
            conversion_count += 1
        
        print(f'INFO: Successfully converted {conversion_count} files to UTF-8')

        # Step 5: Move to target with proper pattern
        print("Step 5: Moving files to target location...")
        
        target_path = f'{param_output_path}Processedfiles/{zip_basename}_{account_date}/'
        
        utf8_files = wr.s3.list_objects(f's3://{bucket_name}/{utf8_workspace}')
        target_count = 0
        
        for file_path in utf8_files:
            filename = file_path.split('/')[-1]
            target_key = f'{target_path}{filename}'
            s3copy(bucket_name, file_path, target_key)
            target_count += 1
        
        print(f'INFO: Successfully moved {target_count} files to target location: {target_path}')

        # Step 6: Archive ZIP with accounting date
        print("Step 6: Archiving ZIP file with accounting date...")
        
        zip_name_without_ext = zipfile_name.split('/')[-1].replace('.ZIP', '').replace('.zip', '')
        archive_key = f'{param_archive_path}{zip_name_without_ext}_ETL_{account_date}.ZIP'
        
        s3copy(bucket_name, temp_zip_key, archive_key)
        print(f'INFO: Successfully archived ZIP file to: {archive_key}')

        # Step 7: Clean up temporary workspace and original files
        print("Step 7: Cleaning up temporary files...")
        
        # Delete entire temporary workspace
        wr.s3.delete_objects(f's3://{bucket_name}/{temp_workspace}')
        
        # Delete original ZIP
        wr.s3.delete_objects(f's3://{bucket_name}/{zipfile_name}')
        
        print('INFO: Successfully cleaned up temporary files')

        print("="*50)
        print("PIPELINE COMPLETED SUCCESSFULLY")
        print("="*50)
        print(f"- Original ZIP: {zipfile_name}")
        print(f"- Accounting Date: {account_date}")
        print(f"- Files Processed: {conversion_count}")
        print(f"- Target Location: {target_path}")
        print(f"- Archive Location: {archive_key}")

    except Exception as ex:
        ex_desc = repr(ex)
        ex_stacktrace = traceback.format_exc()
        error_desc = f'PIPELINE_ERROR: Unexpected error during processing\n\n' \
                 f'Exception: {ex_desc}\n\n{ex_stacktrace}'
        print(error_desc)

        # Final cleanup attempt with reject folder
        if 'temp_workspace' in locals() and temp_workspace:
            move_to_reject_folder(bucket_name, zipfile_name, temp_workspace, param_reject_path, 
                                original_zip_key, temp_zip_key, 'unexpected_error')

        abort_script(error_desc)
