import awswrangler as wr
import zipfile
import boto3
from common_grp4 import *
import traceback
from io import BytesIO
from datetime import datetime
import pandas as pd
from pytz import timezone
import warnings
import chardet

warnings.simplefilter("ignore", UserWarning)

tz = timezone('US/Eastern')
startdt = datetime.now(tz)
load_date = startdt.date().isoformat()
format_snap_date = load_date
rejecttime = startdt.strftime('%Y%m%d%H%M%S')
process_date = startdt.strftime('%Y%m%d')

source_cnt = 0
target_cnt = 0
count = 0

s3_reject_input_prefix = ''
param_input_path = ''
param_load_category = ''
param_task_flowname = ''
new_src_filename = ''
param_account_date_position = 1
account_date = ''

s3 = boto3.resource('s3')
client_s3 = boto3.client('s3')


if __name__ == "__main__":
    try:
        starttime = startdt.strftime('%Y-%m-%dT%H:%M:%SZ')
        print("arg1= ", sys.argv[1])
        print("arg2= ", sys.argv[2])
        print("arg3= ", sys.argv[3])
        print("arg4= ", sys.argv[4])
        print("arg5= ", sys.argv[5])

        error_desc = ''

        bucket_name = sys.argv[1]
        job_id = sys.argv[2]
        task_name = sys.argv[3]
        param_file = sys.argv[4]
        
        zipfile_name = sys.argv[5]

        # Reading Param file in S3
        (param_input_path,
         param_input_path_fileprefix,
         param_expected_files,
         param_reject_path,
         param_output_path,
         param_load_category,
         param_task_flowname,
         param_archive_path,
         param_trailer_processdt_location,
         param_filedelimiter,
         param_specialfilenames,
         param_trailer_grandtotal_location,
         param_specialfiledelimiter,
         param_outofscopetrailerfiles,
         param_trailer_consolidation) = extract_param_value(bucket_name, param_file)

        # Step 1: Extract snapshot date and rename ZIP file
        print("Step 1: Extracting snapshot date and renaming ZIP file...")
        
        src_name = zipfile_name.rsplit('/', 2)[-1]
        src_name_withoutext = zipfile_name.rsplit('/', 2)[-1][:-4]
        trailer_filename = f'{param_trailer_consolidation}'
        key = f'{param_input_path}{src_name}'
        
        zipped_file = client_s3.get_object(Bucket=bucket_name, Key=key)
        buffer = BytesIO(zipped_file["Body"].read())
        z = zipfile.ZipFile(buffer)
        
        with z.open(trailer_filename) as extracted_file:
            lines = extracted_file.read().decode('utf-16').splitlines()
            last_record_utf8 = lines[-1].encode('utf-8')
            data = str(last_record_utf8)
            trailer_df = pd.DataFrame([x.split(param_filedelimiter) for x in data.split('\n')])
            account_date_re = trailer_df.iloc[:, [int(param_account_date_position)]].values[0]
            account_date = str(account_date_re)[2:-2].replace('-', '')
            account_date = account_date.split(' ')[0]
            
        new_src_filename = f'{param_input_path}{src_name_withoutext}_ETL_{account_date}.ZIP'
        s3copy(bucket_name, f'{param_input_path}{src_name}', new_src_filename)
        wr.s3.delete_objects(zipfile_name)
        
        print(f'INFO: The Source file was successfully renamed to {new_src_filename}')

        # Step 2: Unzip the renamed file
        print("Step 2: Unzipping files...")
        
        format_snap_date = datetime.strptime(account_date, '%Y%m%d').date().isoformat()
        folder_name = new_src_filename.split("_ETL_")[0].split('/')[-1]
        output_prefix = f'{param_input_path}{account_date}/unzip/{folder_name}/'
        s3_reject_input_prefix = f'{param_reject_path}{account_date}/{rejecttime}/'

        src_name = str(new_src_filename).rsplit('/', 2)[-1]
        src_name_withoutext = new_src_filename.rsplit('/', 2)[-1][:-4]

        zipped_file = client_s3.get_object(Bucket=bucket_name, Key=f'{param_input_path}{src_name}')
        buffer = BytesIO(zipped_file["Body"].read())
        z = zipfile.ZipFile(buffer)
        
        unzip_count = 0
        for filepath in z.namelist():
            filename = filepath.split('/')[-1]
            unzip_count += 1
            response = s3.meta.client.upload_fileobj(z.open(filepath), bucket_name, Key=f'{output_prefix}{filename}')
        
        print(f'INFO: ALL {unzip_count} files are successfully unzipped')

        # Step 3: Convert files to UTF-8
        print("Step 3: Converting files to UTF-8...")
        s3_input_path = f'{param_input_path}{account_date}/unzip/{folder_name}/'
        s3_reject_input_prefix = f'{param_reject_path}{account_date}/{rejecttime}/'
        s3_output_path = f'{param_output_path}Processedfiles/{folder_name}/'

        s3_results = wr.s3.list_objects('s3://' + bucket_name + '/' + s3_input_path)
        cnt_nooffiles = len(s3_results)

        conversion_count = 0
        conversion_errors = []
        
        for filename in s3_results:
            filename = filename.rsplit('/', 2)[-1]
            file_path = s3_input_path + filename
            
            try:
                # UTF conversion inline
                obj = s3.Object(bucket_name, file_path)
                try:
                    bytes_data = obj.get()['Body'].read(1000).decode('UTF-8')
                    object_content = obj.get()['Body'].read().decode('UTF-8')
                except UnicodeDecodeError:
                    bytes_data = obj.get()['Body'].read(1000)
                    result = chardet.detect(bytes_data)
                    encode = next(iter(result.values()))
                    object_content = obj.get()['Body'].read().decode(encode)
                
                target_filename = f'{s3_output_path}{filename}'
                client_s3.put_object(Body=object_content, Bucket=bucket_name, Key=target_filename)
                conversion_count += 1
            except Exception as exception_error:
                error_desc = f'ERROR: Error occurred while processing {filename} file'
                print('Exception Message : ', exception_error)
                conversion_errors.append(error_desc)

        if len(conversion_errors) > 0:
            # Move files to reject folder if there were conversion errors
            copy_unzip_input_path = param_input_path + account_date + '/'
            copy_zip_path = f'{param_reject_path}{account_date}/'
            s3copy(bucket_name, copy_unzip_input_path, s3_reject_input_prefix)
            wr.s3.delete_objects('s3://' + bucket_name + '/' + copy_unzip_input_path)
            s3copy(bucket_name, param_input_path, copy_zip_path)
            s3copy(bucket_name, param_output_path, s3_reject_input_prefix)
            wr.s3.delete_objects('s3://' + bucket_name + '/' + f'{param_output_path}Processedfiles/')
            
            error_desc = f'ERROR: utf8 conversion failed, the source files were moved to the reject folder.'
            print(error_desc)
            raise Exception(error_desc)

        print(f'INFO: ALL {cnt_nooffiles} files are successfully converted to UTF-8')

        print(f"Pipeline completed successfully:")
        print(f"- Renamed ZIP file: {new_src_filename}")
        print(f"- Unzipped {unzip_count} files")
        print(f"- Converted {conversion_count} files to UTF-8")

    except Exception as ex:
        ex_desc = repr(ex)
        ex_stacktrace = traceback.format_exc()
        error_desc = f'RUNTIME_ERROR: Error occurred during pipeline execution. See full error below\n\n' \
                 f'Exception: {ex_desc}\n\n{ex_stacktrace}'
        print(error_desc)

        # Cleanup on failure - move files to reject folder if possible
        if 's3_reject_input_prefix' in locals():
            try:
                copy_unzip_input_path = param_input_path + account_date + '/'
                copy_zip_path = f'{param_reject_path}{account_date}/'
                s3copy(bucket_name, copy_unzip_input_path, s3_reject_input_prefix)
                wr.s3.delete_objects('s3://' + bucket_name + '/' + copy_unzip_input_path)
                s3copy(bucket_name, param_input_path, copy_zip_path)
                if 'src_name' in locals():
                    wr.s3.delete_objects('s3://' + bucket_name + '/' + param_input_path + src_name)
                print('ERROR: Pipeline failed, files moved to reject folder.')
            except:
                pass  # Ignore cleanup errors

        abort_script(error_desc)
