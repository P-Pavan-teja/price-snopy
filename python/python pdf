import io
import boto3
import pdfplumber
import pandas as pd

_s3 = boto3.client("s3")  # IICS Secure Agent IAM creds

def pdf_to_text(file_path, output_path):
    """
    If file_path/output_path are s3:// URIs, 
    read PDF from S3 and write TXT back to S3.
    """
    lines = []
    if str(file_path).startswith("s3://"):
        bucket, key = file_path[5:].split("/", 1)
        obj = _s3.get_object(Bucket=bucket, Key=key)
        pdf_bytes = obj["Body"].read()

        with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
            for page in pdf.pages:
                txt = page.extract_text()
                if txt:
                    lines.append(txt + "\n")

        if output_path and str(output_path).startswith("s3://"):
            b2, k2 = output_path[5:].split("/", 1)
            _s3.put_object(
                Bucket=b2,
                Key=k2,
                Body="".join(lines).encode("utf-8"),
                ContentType="text/plain"
            )
    else:
        # original local fallback
        with pdfplumber.open(file_path) as pdf, open(output_path, "w", encoding="utf-8") as out:
            for page in pdf.pages:
                txt = page.extract_text()
                if txt:
                    out.write(txt + "\n")
    return lines


def read_text_file(file_path):
    """
    If file_path is s3://..., fetch TXT file from S3 into DataFrame.
    """
    if str(file_path).startswith("s3://"):
        bucket, key = file_path[5:].split("/", 1)
        obj = _s3.get_object(Bucket=bucket, Key=key)
        content = obj["Body"].read().decode("utf-8", errors="replace").splitlines()
        return pd.DataFrame({'Line': [line.strip() for line in content]})
    else:
        with open(file_path, "r", encoding="utf-8", errors="replace") as f:
            return pd.DataFrame({'Line': [line.strip() for line in f]})


def write_csv_to_s3(df: pd.DataFrame, s3_uri: str):
    """
    Upload CSV DataFrame directly to S3.
    """
    bucket, key = s3_uri[5:].split("/", 1)
    buf = io.StringIO()
    df.to_csv(buf, index=False)
    _s3.put_object(
        Bucket=bucket,
        Key=key,
        Body=buf.getvalue().encode("utf-8"),
        ContentType="text/csv"
    )
import io
import snowflake.snowpark as snowpark
from snowflake.snowpark.functions import col

def main(session: snowpark.Session):
    # Example Snowpark DF -> Pandas
    df_sp = session.table('information_schema.packages').filter(col('language') == 'python')
    df_pd = df_sp.to_pandas()

    # 1) Build CSV in text buffer
    text_buf = io.StringIO()
    df_pd.to_csv(text_buf, index=False)
    text_buf.seek(0)

    # 2) Convert to BYTES for put_stream
    bytes_buf = io.BytesIO(text_buf.getvalue().encode('utf-8'))
    bytes_buf.seek(0)

    # 3) Upload with keyword args (stage path MUST be a quoted string)
    session.file.put_stream(
        stage_location="@COMMON_DB.UTIL_SCH.FOUNDRY_TEST/csvfiles/output.csv",
        input_stream=bytes_buf,
        overwrite=True,          # optional
        auto_compress=False      # optional
    )

    return "CSV written to @COMMON_DB.UTIL_SCH.FOUNDRY_TEST/csvfiles/output.csv"
