import re
import tabula
import pdfplumber
import pandas as pd

PDF_PATH = "input.pdf"
START_PAGE = 1
END_PAGE = 1200
BATCH_SIZE = 50   # important for performance on 1200 pages

# Adjust regex if needed (this covers db.sch.tbl with letters/numbers/_/$)
DB_RE = re.compile(r"\b[A-Za-z_][\w$]*\.[A-Za-z_][\w$]*\.[A-Za-z_][\w$]*\b")

def tabula_json_to_df(tjson):
    rows = [[cell.get("text", "") for cell in row] for row in tjson["data"]]
    df = pd.DataFrame(rows)
    df = df.replace(r"^\s*$", pd.NA, regex=True).dropna(how="all").reset_index(drop=True)
    return df

def extract_lines_with_y(plumber_page, y_tol=3):
    words = plumber_page.extract_words()
    if not words:
        return []
    words = sorted(words, key=lambda w: (w["top"], w["x0"]))

    lines = []
    current, current_y = [], None

    for w in words:
        y = w["top"]
        if current_y is None or abs(y - current_y) <= y_tol:
            current.append(w)
            current_y = y if current_y is None else (current_y + y) / 2
        else:
            lines.append({"top": current_y, "text": " ".join(x["text"] for x in current)})
            current, current_y = [w], y

    if current:
        lines.append({"top": current_y, "text": " ".join(x["text"] for x in current)})

    return lines

def find_nearest_db_label_above(lines, table_top, max_scan=350):
    # Find all db.sch.tbl matches above the table within max_scan distance
    best = None  # (y, label)
    for ln in lines:
        if ln["top"] < table_top and (table_top - ln["top"]) <= max_scan:
            m = DB_RE.search(ln["text"])
            if m:
                y = ln["top"]
                if best is None or y > best[0]:  # closest above
                    best = (y, m.group(0))
    return best[1] if best else None

all_parts = []

with pdfplumber.open(PDF_PATH) as pdf:
    for batch_start in range(START_PAGE, END_PAGE + 1, BATCH_SIZE):
        batch_end = min(batch_start + BATCH_SIZE - 1, END_PAGE)
        page_range = f"{batch_start}-{batch_end}"
        print("Processing pages:", page_range)

        # 1) Get tables with geometry and page number
        tables_json = tabula.read_pdf(
            PDF_PATH,
            pages=page_range,
            multiple_tables=True,
            guess=True,
            silent=True,
            output_format="json"
        )

        if not tables_json:
            continue

        # 2) Cache page text lines once per page (critical for performance)
        page_lines_cache = {}

        for t in tables_json:
            page_num = int(t["page"])  # 1-based
            if page_num not in page_lines_cache:
                page_lines_cache[page_num] = extract_lines_with_y(pdf.pages[page_num - 1], y_tol=3)

            lines = page_lines_cache[page_num]
            table_top = float(t["top"])

            label = find_nearest_db_label_above(lines, table_top, max_scan=350) or ""

            df = tabula_json_to_df(t)

            # OPTIONAL: if your tables always have 3 cols, enforce it
            # keep only first 3 columns if extra junk appears
            if df.shape[1] > 3:
                df = df.iloc[:, :3]

            df.columns = ["column_name", "description", "comments"] if df.shape[1] == 3 else df.columns

            df.insert(0, "table_name", label)
            df.insert(1, "__page__", page_num)

            all_parts.append(df)

combined = pd.concat(all_parts, ignore_index=True) if all_parts else pd.DataFrame()
display(combined)

# Save
# combined.to_csv("all_tables_with_table_name.csv", index=False)
